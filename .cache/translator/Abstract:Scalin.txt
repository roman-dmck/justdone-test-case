Анотація: Масштабування обчислень під час тестування є ключовим для підвищення здатностей великих мовних моделей (ВММ) до логічного мислення. Існуючі підходи зазвичай використовують підкріплювальне навчання (ПН) для максимізації перевірюваної винагороди, отриманої наприкінці процесу мислення. Однак такі методи оптимізують лише кінцеву продуктивність за умов великого та фіксованого бюджету токенів, що обмежує ефективність як у процесі навчання, так і при розгортанні. У цій роботі ми представляємо нову структуру — AnytimeReasoner, яка спрямована на оптимізацію продуктивності мислення в будь-який час, з метою підвищення ефективності використання токенів та гнучкості мислення при різних обмеженнях бюджету токенів. Для досягнення цієї мети ми обрізуємо повний процес мислення, щоб він помістився у вибраний бюджет токенів, взятий із попереднього розподілу, змушуючи модель підсумовувати оптимальну відповідь для кожного обрізаного процесу мислення для перевірки. Це вводить перевірювані щільні винагороди у процес мислення, що сприяє більш ефективному розподілу кредитів у процесі ПН. Потім ми оптимізуємо політики мислення та підсумовування у роздільному режимі для максимізації сукупної винагороди. Крім того, ми впроваджуємо нову техніку зменшення дисперсії — Budget Relative Policy Optimization (BRPO), яка підвищує стійкість і ефективність процесу навчання при підкріпленні політики мислення. Емпіричні результати на задачах математичного мислення демонструють, що наш метод стабільно перевищує GRPO за всіма бюджетами мислення при різних попередніх розподілах, покращуючи як навчання, так і ефективність використання токенів.